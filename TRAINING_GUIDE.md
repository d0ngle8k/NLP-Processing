# ğŸš€ HÆ°á»›ng Dáº«n Fine-tune PhoBERT

## ğŸ“Š Tá»•ng Quan

File nÃ y hÆ°á»›ng dáº«n fine-tune PhoBERT model Ä‘á»ƒ cáº£i thiá»‡n event extraction accuracy.

### ğŸ“ˆ Hiá»‡u Suáº¥t Hiá»‡n Táº¡i
- **Rule-based**: 96.88% Macro F1
- **PhoBERT (base)**: 71.43% Macro F1 âŒ
- **Hybrid**: 96.88% Macro F1 (dá»±a vÃ o rule-based)

### ğŸ¯ Má»¥c TiÃªu Sau Fine-tuning
- **PhoBERT (fine-tuned)**: 90%+ Macro F1 âœ…
- **Hybrid**: 98%+ Macro F1 âœ…

---

## ğŸ› ï¸ PhÆ°Æ¡ng PhÃ¡p 1: Google Colab (Khuyáº¿n nghá»‹ â­)

### âœ… Æ¯u Ä‘iá»ƒm
- **Miá»…n phÃ­ T4 GPU** (15GB VRAM)
- **Nhanh hÆ¡n CPU 50x**: ~30-60 phÃºt thay vÃ¬ 51 giá»
- KhÃ´ng cáº§n cÃ i Ä‘áº·t CUDA local

### ğŸ“ CÃ¡c BÆ°á»›c

#### 1. Má»Ÿ Google Colab
- Truy cáº­p: https://colab.research.google.com
- File â†’ Upload notebook â†’ Chá»n `colab_training.ipynb`

#### 2. Báº­t GPU
- Runtime â†’ Change runtime type
- Hardware accelerator â†’ **T4 GPU**
- Save

#### 3. Cháº¡y tá»«ng cell
```python
# Cell 1: Clone repo
!git clone https://github.com/d0ngle8k/NLP-Processing.git
%cd NLP-Processing

# Cell 2: Install dependencies
!pip install -q torch transformers underthesea tqdm

# Cell 3: Check GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")

# Cell 4: Training (30-60 phÃºt)
!python train_phobert.py --epochs 3 --batch_size 16

# Cell 5: Download model
!zip -r phobert_finetuned.zip models/phobert_finetuned
from google.colab import files
files.download('phobert_finetuned.zip')
```

#### 4. Sau khi download
```bash
# Giáº£i nÃ©n vÃ o thÆ° má»¥c models/
unzip phobert_finetuned.zip

# Test model
python comprehensive_test.py

# Commit
git add models/phobert_finetuned
git commit -m "v1.1.0: Add fine-tuned PhoBERT model"
git push
```

---

## ğŸ’» PhÆ°Æ¡ng PhÃ¡p 2: Local Training (Cháº­m âš ï¸)

### âš™ï¸ YÃªu Cáº§u
- **GPU**: NVIDIA GPU vá»›i CUDA support
- **RAM**: 16GB+
- **Storage**: 5GB+ free space

### ğŸ“ CÃ¡c BÆ°á»›c

#### 1. CÃ i Ä‘áº·t CUDA (náº¿u cÃ³ GPU)
```powershell
# Check GPU
nvidia-smi

# Install PyTorch vá»›i CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### 2. Training
```powershell
# Full training (3 epochs)
python train_phobert.py --epochs 3 --batch_size 16

# Hoáº·c test vá»›i dataset nhá» hÆ¡n
python train_phobert.py --epochs 1 --batch_size 8
```

#### 3. Monitor Progress
```powershell
# Terminal khÃ¡c
python monitor_training.py
```

### â±ï¸ Thá»i Gian Æ¯á»›c TÃ­nh
- **CPU**: ~51 giá» (3 epochs) âŒ
- **GPU (RTX 3060)**: ~2-3 giá» (3 epochs) âš¡
- **GPU (RTX 4090)**: ~30-60 phÃºt (3 epochs) âš¡âš¡

---

## ğŸ“Š Training Data

### ğŸ“ Datasets
```
training_data/
â”œâ”€â”€ phobert_training_augmented.json  (95K+ samples) â† Sá»­ dá»¥ng file nÃ y
â”œâ”€â”€ phobert_train.json               (772K samples, too large)
â””â”€â”€ phobert_validation.json          (85K samples)
```

### ğŸ“ˆ Dataset Statistics
- **Training**: 76,266 samples
- **Validation**: 19,067 samples
- **Coverage**: Week/month reminders, location conflicts, edge cases

---

## ğŸ›ï¸ Training Options

### Command Line Arguments
```bash
python train_phobert.py [OPTIONS]

Options:
  --epochs INT          Number of epochs (default: 5)
  --batch_size INT      Batch size (default: 16, giáº£m xuá»‘ng 8 hoáº·c 4 náº¿u OOM)
  --lr FLOAT           Learning rate (default: 2e-5)
  --output PATH        Output directory (default: ./models/phobert_finetuned)
  --skip_checks        Skip requirement checks
```

### Examples
```bash
# Basic (recommended)
python train_phobert.py --epochs 3 --batch_size 16

# Custom learning rate
python train_phobert.py --epochs 5 --lr 1e-5

# Small batch for limited GPU
python train_phobert.py --epochs 3 --batch_size 4

# Save to custom location
python train_phobert.py --output ./my_models/phobert_v2
```

---

## ğŸ“Š Monitoring Training

### Real-time Progress
```powershell
# Terminal 1: Training
python train_phobert.py --epochs 3 --batch_size 16

# Terminal 2: Monitor
python monitor_training.py
```

### Training Logs
```
models/phobert_finetuned/
â”œâ”€â”€ training.log          # Training progress
â”œâ”€â”€ pytorch_model.bin     # Model weights
â”œâ”€â”€ config.json          # Model config
â””â”€â”€ tokenizer files...
```

### Expected Output
```
ğŸ“Š Epoch 1/3
Training: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9534/9534 [30:21, 5.23it/s, loss=0.45]
Validation: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2384/2384 [03:12, 12.4it/s]
   Train Loss: 0.4532
   Val Loss: 0.3821
   Val Accuracy: 89.3%
```

---

## âœ… Testing Fine-tuned Model

### Run Tests
```powershell
# Test all pipelines
python comprehensive_test.py
```

### Expected Improvements
| Component | Before (base) | After (fine-tuned) | Improvement |
|-----------|---------------|-------------------|-------------|
| Event     | 0% F1         | 90%+ F1          | +90%        |
| Time      | 0% F1         | 85%+ F1          | +85%        |
| Location  | 0% F1         | 80%+ F1          | +80%        |
| Reminder  | 0% F1         | 85%+ F1          | +85%        |
| **Macro F1** | **71.43%** | **90%+**        | **+18.57%** |

---

## ğŸ› Troubleshooting

### Out of Memory (OOM)
```bash
# Giáº£m batch size
python train_phobert.py --batch_size 4

# Hoáº·c dÃ¹ng gradient accumulation (trong code)
```

### CUDA Not Available
```powershell
# Check CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Install CUDA version of PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Slow Training
```
âœ… Sá»­ dá»¥ng Google Colab vá»›i T4 GPU (miá»…n phÃ­)
âœ… Giáº£m dataset size cho testing
âœ… Increase batch_size náº¿u cÃ³ GPU memory
```

### Import Errors
```powershell
# Reinstall dependencies
pip install torch transformers underthesea tqdm
```

---

## ğŸ“¦ Model Deployment

### 1. Save Model
Model tá»± Ä‘á»™ng save táº¡i: `./models/phobert_finetuned/`

### 2. Use in Pipeline
```python
from core_nlp.hybrid_pipeline import HybridPipeline

# Load fine-tuned model
pipeline = HybridPipeline(model_path="./models/phobert_finetuned")

# Extract event
result = pipeline.extract("mai 8h há»p á»Ÿ phÃ²ng 302")
print(result)
```

### 3. Git Commit
```bash
# Add model files
git add models/phobert_finetuned

# Commit
git commit -m "v1.1.0: Add fine-tuned PhoBERT model

- Trained on 76K+ augmented samples
- Improved Macro F1: 71.43% â†’ 90%+
- Full week/month reminder support"

# Push
git push
```

---

## ğŸ“š Resources

### Documentation
- **PhoBERT Paper**: https://arxiv.org/abs/2003.00744
- **Transformers Docs**: https://huggingface.co/docs/transformers
- **Training Guide**: `core_nlp/phobert_trainer.py`

### Files
- **Training Script**: `train_phobert.py`
- **Colab Notebook**: `colab_training.ipynb`
- **Monitor Script**: `monitor_training.py`
- **Test Script**: `comprehensive_test.py`

### Support
- GitHub Issues: https://github.com/d0ngle8k/NLP-Processing/issues
- Training Logs: `models/phobert_finetuned/training.log`

---

## ğŸ¯ Next Steps

1. âœ… **Báº­t GPU trÃªn Colab** (náº¿u dÃ¹ng Colab)
2. âœ… **Run training** vá»›i `train_phobert.py`
3. âœ… **Download model** tá»« Colab
4. âœ… **Test improvements** vá»›i `comprehensive_test.py`
5. âœ… **Commit to Git** vÃ  deploy

**Estimated Total Time**: 30-60 phÃºt (Colab GPU) hoáº·c 2-3 giá» (Local GPU)

Good luck! ğŸš€
