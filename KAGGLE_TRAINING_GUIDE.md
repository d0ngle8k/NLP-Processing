# ğŸš€ HÆ°á»›ng Dáº«n Train PhoBERT trÃªn Kaggle

## ğŸ“Š Táº¡i Sao Chá»n Kaggle?

### âš¡ So SÃ¡nh Platforms

| Feature | Kaggle | Google Colab | Local RTX 2060 |
|---------|--------|--------------|----------------|
| **GPU** | P100 (16GB) | T4 (15GB) | RTX 2060 (8GB) |
| **RAM** | 30GB | 12GB | 16GB |
| **Miá»…n phÃ­** | âœ… | âœ… | âœ… (Ä‘Ã£ cÃ³) |
| **Thá»i gian/tuáº§n** | 30 giá» | Variable | Unlimited |
| **Tá»‘c Ä‘á»™ training** | âš¡âš¡âš¡ Nhanh nháº¥t | âš¡âš¡ Trung bÃ¬nh | âš¡ Cháº­m nháº¥t |
| **Æ¯á»›c tÃ­nh (3 epochs)** | **20-30 phÃºt** | 30-40 phÃºt | 45-60 phÃºt |

### ğŸ¯ Khuyáº¿n Nghá»‹: **KAGGLE** 
- GPU máº¡nh hÆ¡n (P100 > T4 > RTX 2060)
- RAM nhiá»u hÆ¡n (30GB)
- á»”n Ä‘á»‹nh hÆ¡n
- Interface thÃ¢n thiá»‡n

---

## ğŸ“ CÃ¡c BÆ°á»›c Thá»±c Hiá»‡n

### ğŸ”‘ BÆ°á»›c 1: Táº¡o TÃ i Khoáº£n Kaggle

1. Truy cáº­p: https://www.kaggle.com
2. Click **Register** (gÃ³c trÃªn bÃªn pháº£i)
3. ÄÄƒng kÃ½ vá»›i:
   - Google account, hoáº·c
   - Email + Password
4. XÃ¡c nháº­n email

### ğŸ“± BÆ°á»›c 2: Verify Phone Number (Äá»ƒ dÃ¹ng GPU)

âš ï¸ **Quan trá»ng**: Kaggle yÃªu cáº§u verify sá»‘ Ä‘iá»‡n thoáº¡i Ä‘á»ƒ dÃ¹ng GPU

1. Click vÃ o **avatar** (gÃ³c trÃªn bÃªn pháº£i)
2. Chá»n **Settings**
3. Tab **Phone Verification**
4. Nháº­p sá»‘ Ä‘iá»‡n thoáº¡i (+84 xxx xxx xxx)
5. Nháº­p mÃ£ OTP
6. âœ… Verified â†’ BÃ¢y giá» cÃ³ thá»ƒ dÃ¹ng GPU!

### ğŸ“Š BÆ°á»›c 3: Táº¡o Notebook Má»›i

1. VÃ o: https://www.kaggle.com/code
2. Click **New Notebook**
3. Hoáº·c: Click **Create** â†’ **New Notebook**

### ğŸ“¥ BÆ°á»›c 4: Upload Notebook

#### Option 1: Upload tá»« file
1. Click **File** â†’ **Import Notebook**
2. Chá»n tab **Upload**
3. KÃ©o tháº£ file `kaggle_training.ipynb`
4. Click **Import**

#### Option 2: Táº¡o tá»« URL
1. Click **File** â†’ **Import Notebook**
2. Chá»n tab **GitHub**
3. Paste URL: `https://github.com/d0ngle8k/NLP-Processing/blob/main/kaggle_training.ipynb`
4. Click **Import**

### ğŸ® BÆ°á»›c 5: Báº­t GPU

âš¡ **QUAN TRá»ŒNG NHáº¤T!**

1. Click **Settings** (âš™ï¸ icon bÃªn pháº£i)
2. TÃ¬m pháº§n **Accelerator**
3. Chá»n:
   - **GPU P100** (náº¿u cÃ³ - nhanh nháº¥t âš¡âš¡âš¡) HOáº¶C
   - **GPU T4 x2** (náº¿u P100 khÃ´ng cÃ³ - váº«n nhanh âš¡âš¡)
4. Click **Save** (gÃ³c dÆ°á»›i)
5. âœ… Notebook sáº½ restart vá»›i GPU

### â–¶ï¸ BÆ°á»›c 6: Cháº¡y Training

#### 6.1. Run tá»«ng cell theo thá»© tá»±:

**Cell 1: Clone Repository**
```python
!git clone https://github.com/d0ngle8k/NLP-Processing.git
%cd NLP-Processing
```
â±ï¸ Thá»i gian: ~5 giÃ¢y

**Cell 2: Install Dependencies**
```python
!pip install -q torch transformers underthesea tqdm
```
â±ï¸ Thá»i gian: ~30 giÃ¢y

**Cell 3: Check GPU**
```python
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
```
âœ… Pháº£i tháº¥y: "Tesla P100-PCIE-16GB" hoáº·c "Tesla T4"

**Cell 4: Check Training Data**
```python
import json
with open('training_data/phobert_training_augmented.json') as f:
    data = json.load(f)
print(f"Total samples: {len(data):,}")
```
âœ… Pháº£i tháº¥y: "Total samples: 95,332"

**Cell 5: START TRAINING** ğŸš€
```python
!python train_phobert.py --epochs 3 --batch_size 16
```
â±ï¸ Thá»i gian: **20-30 phÃºt** (P100) hoáº·c **30-40 phÃºt** (T4)

#### 6.2. Theo dÃµi progress:

Báº¡n sáº½ tháº¥y:
```
ğŸ“Š Epoch 1/3
------------------------------------------------------------
Training: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4767/4767 [08:23<00:00, 9.47it/s, loss=0.423]
Validation: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1192/1192 [01:12<00:00, 16.5it/s]
   Train Loss: 0.4231
   Val Loss: 0.3654
   Val Accuracy: 88.7%

ğŸ“Š Epoch 2/3
------------------------------------------------------------
Training: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4767/4767 [08:21<00:00, 9.51it/s, loss=0.287]
...
```

âœ… **Chá» Ä‘áº¿n khi training xong** (3/3 epochs complete)

### ğŸ“¦ BÆ°á»›c 7: Download Model

**Cell 6: Create ZIP**
```python
!zip -r phobert_finetuned.zip models/phobert_finetuned/
```

**Cell 7: Check File**
```python
!ls -lh phobert_finetuned.zip
```
âœ… File size: ~800MB - 1.5GB

#### Download file:

**Method 1: Kaggle Output Panel** (Recommended)
1. Xem panel **Output** bÃªn pháº£i
2. TÃ¬m file `phobert_finetuned.zip`
3. Click icon download â¬‡ï¸
4. Chá» download (800MB - 1.5GB)

**Method 2: Kaggle API** (Alternative)
```bash
# Install Kaggle CLI
pip install kaggle

# Download tá»« terminal
kaggle kernels output <username>/<notebook-name> -p ./
```

---

## ğŸ“¥ Sau Khi Download

### 1. Giáº£i NÃ©n Model

**Windows PowerShell:**
```powershell
cd "C:\Users\d0ngle8k\Desktop\New folder (2)\NLP-Processing"

# Giáº£i nÃ©n
Expand-Archive -Path .\phobert_finetuned.zip -DestinationPath . -Force
```

**Linux/Mac:**
```bash
cd ~/NLP-Processing
unzip phobert_finetuned.zip
```

### 2. Verify Model Files

```powershell
ls models\phobert_finetuned
```

Pháº£i cÃ³:
```
pytorch_model.bin      (800MB - 1.2GB)
config.json
tokenizer_config.json
vocab.txt
training.log
```

### 3. Test Model

```powershell
python comprehensive_test.py
```

Káº¿t quáº£ mong Ä‘á»£i:
```
PhoBERT Pipeline Results:
   Event F1: 0.90+ (90%+)
   Time F1: 0.85+ (85%+)
   Location F1: 0.80+ (80%+)
   Reminder F1: 0.85+ (85%+)
   Macro F1: 0.90+ (90%+)  â¬†ï¸ from 71.43%
```

### 4. Commit lÃªn GitHub

```powershell
# Add model files
git add models/phobert_finetuned

# Commit
git commit -m "v1.1.0: Add fine-tuned PhoBERT model

Trained on Kaggle P100 GPU
- Training samples: 76,266
- Validation samples: 19,067
- Epochs: 3
- Training time: ~25 minutes
- Macro F1: 71.43% â†’ 90%+ (+18.57%)

Components:
- Event extraction: 90%+ F1
- Time extraction: 85%+ F1
- Location extraction: 80%+ F1
- Reminder extraction: 85%+ F1"

# Push
git push origin main
```

### 5. Update README

ThÃªm vÃ o `README.md`:
```markdown
## ğŸ¤– Model Performance

| Pipeline | Macro F1 | Event | Time | Location | Reminder |
|----------|----------|-------|------|----------|----------|
| Rule-based | 96.88% | 100% | 100% | 87.5% | 100% |
| **PhoBERT** | **90%+** | **90%+** | **85%+** | **80%+** | **85%+** |
| Hybrid | 98%+ | 100% | 100% | 90%+ | 100% |

âœ¨ PhoBERT fine-tuned on 76K+ Vietnamese event extraction samples
```

---

## ğŸ› Troubleshooting

### âŒ GPU Not Available

**Váº¥n Ä‘á»:** Cell 3 hiá»ƒn thá»‹ "CUDA not available"

**Giáº£i phÃ¡p:**
1. Check Settings â†’ Accelerator â†’ Pháº£i lÃ  **GPU P100** hoáº·c **GPU T4**
2. Náº¿u lÃ  "None" â†’ Click vÃ  chá»n GPU
3. Click **Save** vÃ  notebook sáº½ restart
4. Run láº¡i cÃ¡c cells

### âŒ Out of Memory (OOM)

**Váº¥n Ä‘á»:** Training bá»‹ crash vá»›i lá»—i "CUDA out of memory"

**Giáº£i phÃ¡p:**
```python
# Giáº£m batch size tá»« 16 xuá»‘ng 12
!python train_phobert.py --epochs 3 --batch_size 12

# Hoáº·c xuá»‘ng 8 náº¿u váº«n OOM
!python train_phobert.py --epochs 3 --batch_size 8
```

### âŒ Training Data Not Found

**Váº¥n Ä‘á»:** "File not found: training_data/phobert_training_augmented.json"

**Giáº£i phÃ¡p:**
1. Verify file tá»“n táº¡i trÃªn GitHub: https://github.com/d0ngle8k/NLP-Processing/tree/main/training_data
2. Náº¿u khÃ´ng cÃ³ â†’ Clone láº¡i repo:
   ```python
   !rm -rf NLP-Processing
   !git clone https://github.com/d0ngle8k/NLP-Processing.git
   %cd NLP-Processing
   ```

### âŒ ZIP File KhÃ´ng Download ÄÆ°á»£c

**Giáº£i phÃ¡p 1:** Download tá»« Kaggle Output panel
1. Scroll xuá»‘ng Output panel bÃªn pháº£i
2. TÃ¬m file `phobert_finetuned.zip`
3. Click icon download

**Giáº£i phÃ¡p 2:** Download tá»«ng file
```python
# List files
!ls models/phobert_finetuned/

# Copy to /kaggle/working/ (accessible from Output)
!cp -r models/phobert_finetuned /kaggle/working/
```

### âŒ Phone Verification Required

**Váº¥n Ä‘á»:** "Phone verification required to use GPU"

**Giáº£i phÃ¡p:**
1. Settings â†’ Phone Verification
2. Nháº­p sá»‘ Ä‘iá»‡n thoáº¡i (+84...)
3. Nháº­p OTP code
4. Refresh notebook

---

## ğŸ“Š Monitoring Training

### Xem Real-time Progress

Trong khi training, báº¡n sáº½ tháº¥y:

```
ğŸ“Š Epoch 1/3
------------------------------------------------------------
Training:  45% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 2150/4767 [03:47<04:36, 9.47it/s, loss=0.512]
```

**Giáº£i thÃ­ch:**
- `45%`: ÄÃ£ complete 45% epoch 1
- `2150/4767`: Iteration hiá»‡n táº¡i / tá»•ng iterations
- `[03:47<04:36]`: ÄÃ£ cháº¡y 3:47, cÃ²n 4:36
- `9.47it/s`: Tá»‘c Ä‘á»™ 9.47 iterations/giÃ¢y
- `loss=0.512`: Training loss hiá»‡n táº¡i

### Expected Timeline (P100 GPU)

```
Cell 1 (Clone):        ~5 giÃ¢y
Cell 2 (Install):      ~30 giÃ¢y
Cell 3 (Check GPU):    ~2 giÃ¢y
Cell 4 (Check Data):   ~3 giÃ¢y
Cell 5 (Training):     ~20-30 phÃºt
  - Epoch 1:           ~8 phÃºt
  - Epoch 2:           ~8 phÃºt
  - Epoch 3:           ~8 phÃºt
  - Validation:        ~2 phÃºt/epoch
Cell 6 (Create ZIP):   ~10 giÃ¢y
Cell 7 (Download):     ~2-5 phÃºt (tÃ¹y máº¡ng)

Total: ~25-40 phÃºt
```

---

## ğŸ¯ Best Practices

### 1. Save Notebook Frequently
- **Ctrl+S** hoáº·c **Cmd+S** Ä‘á»ƒ save
- Kaggle auto-save má»—i vÃ i phÃºt

### 2. Run Cells in Order
- Äá»«ng skip cells
- Run tá»« trÃªn xuá»‘ng dÆ°á»›i

### 3. Monitor GPU Usage
```python
# Add cell Ä‘á»ƒ check GPU usage
!nvidia-smi
```

### 4. Download Model Ngay Sau Training
- Kaggle session cÃ³ thá»ƒ timeout
- Download ngay khi training xong

### 5. Keep Notebook Running
- Äá»«ng Ä‘Ã³ng tab browser
- Kaggle cÃ³ thá»ƒ kill inactive sessions

---

## ğŸ“š Resources

### Kaggle Links
- **Kaggle Homepage**: https://www.kaggle.com
- **Notebooks**: https://www.kaggle.com/code
- **Documentation**: https://www.kaggle.com/docs

### Project Links
- **GitHub Repo**: https://github.com/d0ngle8k/NLP-Processing
- **Kaggle Notebook**: `kaggle_training.ipynb`
- **Training Guide**: `TRAINING_GUIDE.md`

### Support
- **Kaggle Forums**: https://www.kaggle.com/discussions
- **GitHub Issues**: https://github.com/d0ngle8k/NLP-Processing/issues

---

## âœ… Checklist

### TrÆ°á»›c Khi Báº¯t Äáº§u:
- [ ] CÃ³ tÃ i khoáº£n Kaggle
- [ ] ÄÃ£ verify phone number
- [ ] ÄÃ£ push code lÃªn GitHub
- [ ] Training data cÃ³ trÃªn GitHub

### Trong Khi Training:
- [ ] Notebook Ä‘Ã£ báº­t GPU (P100 hoáº·c T4)
- [ ] Cell 3 confirm GPU available
- [ ] Training Ä‘ang cháº¡y (xem progress bar)
- [ ] Loss Ä‘ang giáº£m dáº§n

### Sau Training:
- [ ] Training completed (3/3 epochs)
- [ ] ÄÃ£ táº¡o ZIP file
- [ ] ÄÃ£ download vá» local
- [ ] ÄÃ£ giáº£i nÃ©n vÃ o `models/phobert_finetuned/`
- [ ] ÄÃ£ test vá»›i `comprehensive_test.py`
- [ ] Káº¿t quáº£ F1 > 90%
- [ ] ÄÃ£ commit lÃªn GitHub
- [ ] ÄÃ£ update README.md

---

## ğŸŠ Káº¿t Quáº£ Mong Äá»£i

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

âœ… **Fine-tuned PhoBERT model**
- 800MB - 1.5GB model files
- 90%+ Macro F1 score
- Production-ready

âœ… **Improved Pipeline Performance**
```
Before:  PhoBERT: 71.43%  |  Hybrid: 96.88%
After:   PhoBERT: 90%+    |  Hybrid: 98%+
         +18.57%               +1.12%
```

âœ… **Deployment Ready**
- Model trÃªn GitHub
- Documentation updated
- Test results validated

---

**ğŸš€ Ready? Báº¯t Ä‘áº§u train trÃªn Kaggle ngay!**

1. Má»Ÿ https://www.kaggle.com
2. Upload `kaggle_training.ipynb`
3. Báº­t GPU P100
4. Run all cells
5. Äá»£i 25 phÃºt
6. Download model
7. Deploy! ğŸ‰
